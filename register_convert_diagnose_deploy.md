# Tutorial: register, convert, diagnose and deploy a model step by step

In this tutorial, we demonstrate modelci, a toolbox providing APIs that fetch, convert diagnose and deploy pre-trained 
model in different form of variant.

Firstly, make sure that you have started a MongoDB service and configured the MongoDB environment. See 
[installation](/README.md#installation).

## 1. Register a Pre-trained Model

### With a simple configuration file [[template]](/example#register_resnet50_pytorch.yml)

```python
register_model_from_yaml("/example/resnet50_explicit_path.yml")
```

We can register a pre-trained model using `modelci.hub.manager.register_model(...)`. This API has two modes:

- `auto_generate`:

    Enabled by setting `no_generate=False` (default), which converts your model (a PyTorch `nn.Module` or a
    TensorFlow `keras.Model` object) into all possible model family.
- `no_generate`:

    Enabled by setting `no_generate=True`. This will let user register the given model only.

There is a short cut of generation, we can save the model in a standard form:
(See [Tricks with Model Saved Path](#6-tricks-with-model-saved-path))  
`~/.modelci/<model name>/<framework>-<engine>/<version>.<extension>`
In this case, we are only able to specify the path, without architecture, framework, engine and version.

```python
from modelci.hub.manager import register_model
from modelci.persistence.bo.model_objects import IOShape

register_model(
    '~/.modelci/ResNet50/pytorch-torchscript/1.zip',
    dataset='ImageNet',
    acc=0.76,
    task='image classification',
    inputs=[IOShape([-1, 3, 224, 224], float)],
    outputs=[IOShape([-1, 1000], float)],
    no_generate=True
)
```

For quick start (conversion + registration), run
```shell script
python init_data.py export --model {MODEL_NAME} --framework {FRAMEWORK}
```

Currently supported (tested) model name:
- ResNet50

### 1.1 Registration using `auto_generate` mode

```python
import torch.hub

from modelci.hub.manager import register_model
from modelci.persistence.bo.model_objects import IOShape, Framework, ModelVersion
from modelci.utils.trtis_objects import ModelInputFormat

model = torch.hub.load('pytorch/torchvision:v0.5.0', model='resnet50', pretrained=True)
inputs = [IOShape(shape=[-1, 3, 224, 224], dtype=float, format=ModelInputFormat.FORMAT_NCHW)]
outputs = [IOShape(shape=[-1, 1000], dtype=float)]

register_model(
    model,
    dataset='ImageNet',
    acc=0.76,
    task='image classification',
    inputs=inputs,
    outputs=outputs,
    architecture='ResNet50',
    framework=Framework.PYTORCH,
    version=ModelVersion(1)
)
```

### 1.2 Registration using `no_generate` Mode

Assume we have a saved pre-trained ResNet50 model at current working directory named `1.zip`. It was trained on 
ImageNet and exported by TorchScript.

```python
from modelci.hub.manager import register_model
from modelci.persistence.bo import Framework, IOShape, Engine, ModelVersion

register_model(
    'path/to/model/1.zip',
    dataset='ImageNet',
    acc=0.76,
    task='image classification',
    inputs=[IOShape([-1, 3, 224, 224], float)],
    outputs=[IOShape([1], int)],
    architecture='ResNet50',
    framework=Framework.PYTORCH,
    engine=Engine.TORCHSCRIPT,
    version=ModelVersion(1),
    no_generate=True
)
```

## 2. Convert Model to Other Serving Engine Format

modelci provides model conversion between different serving engine based members. These APIs will save the converted model in the given `saved_path` and return success or failure status of the conversion. We restrict to use the standard `saved_path` generated by `modelci.hub.utils.generate_path(...)` or path according to the rules (See [Tricks with Model Saved Path](#6-tricks-with-model-saved-path)). This path format can make your life easier.

### 2.1 TorchScript Conversion


```python
from modelci.hub.converter import TorchScriptConverter

torch_module = ...
saved_path = ...

TorchScriptConverter.from_torch_module(torch_module, saved_path)
```

### 2.2 TensorFlow Serving Conversion

```python
from modelci.hub.converter import TFSConverter

tf_module = ...
saved_path = ...

TFSConverter.from_tf_model(tf_module, saved_path)
```

### 2.3 ONNX Conversion

```python
from modelci.hub.converter import ONNXConverter

torch_module = ...
saved_path = ...
input_shape = ...
batch_size = ...

ONNXConverter.from_torch_module(torch_module, saved_path, input_shape, batch_size)
```

### 2.4 TRT Conversion

From TF Savedmodel to TF-TRT

```python
from modelci.hub.converter import TRTConverter
from modelci.persistence.bo import IOShape

tf_path = ...
trt_path = ...
inputs = [IOShape([...], dtype=..., format=...), ...]
outputs = [IOShape([...], dtype=..., format=...), ...]

TRTConverter.from_saved_model(tf_path, trt_path, inputs=inputs, outputs=outputs)
```

From ONNX to TRT

```python
from modelci.hub.converter import TRTConverter
from modelci.persistence.bo import IOShape

onnx_path = ...
save_path = ...
inputs = [IOShape([...], dtype=...), ...]
outputs = [IOShape([...], dtype=...), ...]

TRTConverter.from_onnx(onnx_path, save_path, inputs=inputs, outputs=outputs)
```

## 3. Diagnose Model

After conversion, the system will deploy the model to avaliable devices and run diagnose to test their performance

## 4. Deploy the Cached Model

Deploy a model as a service listening on specific ports. This function will use a local cached model (obtained by 
`modelci.hub.manager.retrieve_model_by_xxx`, see [Query a model](#4.1-Retrieval)).

We use the auto-generated `saved_path` (See [Tricks with Model Saved Path](#6.-Tricks-with-Model-Saved-Path)) to specify 
model local cache. A serving device can also be assigned using device name (i.e. `'cpu'`, `'cuda:0'`, `'cuda:0,1'`)  

```python
from modelci.hub.deployer import serve

saved_path = ...
device = 'cpu'

serve(saved_path, device)
```

## 5. Manage Models



### 5.1 Retrieve

- [ ] list all models

We can query the Model Hub for uploaded models.

```python
from modelci.hub.manager import retrieve_model_by_name, retrieve_model_by_task
from modelci.persistence.bo import Framework, Engine

# By model name and optionally filtered by model framework and(or) model engine
saved_path, info = retrieve_model_by_name(
    architecture_name='ResNet50', framework=Framework.PYTORCH, engine=Engine.TORCHSCRIPT
)
# By task
saved_path2, info2 = retrieve_model_by_task(task='image classification')
```

The returned tuple contains the local model cached path and model meta information (e.g. model name, model framework).  

Additionally, we can extract model information from the default saved path using the utility function 
`modelci.hub.utils.parse_path`. See [Tricks with Model Saved Path](#6.-Tricks-with-Model-Saved-Path)

**TODO: If we retrieve models by task, we will get a lot of models. These models will be cached in a Redis database for 
further model selection scheduling.**

### 5.2 Update

### 5.3 Delete

## 6. Tricks with Model Saved Path

The default model local cache path is in the following form:  
`~/.modelci/<model name>/<framework>-<engine>/<version>.<extension>`  

We can use `modelci.hub.utils.parse_path(...)` to extract model identification.

```python
from modelci.hub.utils import parse_path

# from return value of model retrieval
saved_path, _ = (..., ...)

info = parse_path(saved_path)

```

The extracted information is a dictionary containing:

```bash
{
    'architecture': architecture,
    'framework': framework,
    'engine': engine,
    'version': version,
    'filename': filename
}
```

Vice versa, we can generate the default path by `modelci.hub.utils.generate_path(...)`:

```python
from modelci.hub.utils import generate_path
from modelci.persistence.bo import Framework, Engine

saved_path = generate_path(
    model_name='ResNet50', framework=Framework.PYTORCH, engine=Engine.TORCHSCRIPT, version=1
)
```
