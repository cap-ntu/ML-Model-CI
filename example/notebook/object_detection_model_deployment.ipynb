{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Object Detection Model Use ModelCI\n",
    "\n",
    "MMDetction is a well-known open source object detection toolbox based on PyTorch. You can refer to <https://arxiv.org/abs/1906.07155> for more details.\n",
    "\n",
    "By walking through this tutorial, you will be able to:\n",
    "\n",
    "- Load pretained MMDetction model\n",
    "- Convert MMDetction model into ONNX format \n",
    "- Register and retrieve models by ModelHub\n",
    "\n",
    "## 1. Prequisities\n",
    " \n",
    "### 1.1 Installation of MMDetction\n",
    " \n",
    " Firstly you have to install MMDetction according to official instructions : <https://mmdetection.readthedocs.io/en/latest/get_started.html#installation> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Looking in links: https://download.openmmlab.com/mmcv/dist/cu102/torch1.8.0/index.html\n",
      "Collecting mmcv-full\n",
      "  Downloading https://download.openmmlab.com/mmcv/dist/cu102/torch1.8.0/mmcv_full-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (16.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.6 MB 231 kB/s \n",
      "\u001b[?25hRequirement already satisfied: yapf in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmcv-full) (0.31.0)\n",
      "Requirement already satisfied: Pillow in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmcv-full) (8.2.0)\n",
      "Requirement already satisfied: opencv-python>=3 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmcv-full) (4.5.1.48)\n",
      "Requirement already satisfied: addict in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmcv-full) (2.4.0)\n",
      "Requirement already satisfied: numpy in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmcv-full) (1.20.1)\n",
      "Requirement already satisfied: pyyaml in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmcv-full) (5.3.1)\n",
      "Installing collected packages: mmcv-full\n",
      "Successfully installed mmcv-full-1.3.2\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///home/modelci/NTU/ML-Model-CI/example/notebook/mmdetection\n",
      "Requirement already satisfied: matplotlib in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmdet==2.11.0) (3.4.1)\n",
      "Requirement already satisfied: numpy in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmdet==2.11.0) (1.20.1)\n",
      "Requirement already satisfied: six in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmdet==2.11.0) (1.15.0)\n",
      "Requirement already satisfied: terminaltables in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmdet==2.11.0) (3.1.0)\n",
      "Requirement already satisfied: pycocotools in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from mmdet==2.11.0) (2.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from matplotlib->mmdet==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from matplotlib->mmdet==2.11.0) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from matplotlib->mmdet==2.11.0) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from matplotlib->mmdet==2.11.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from matplotlib->mmdet==2.11.0) (2.8.1)\n",
      "Requirement already satisfied: cython>=0.27.3 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from pycocotools->mmdet==2.11.0) (0.29.23)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/modelci/miniconda3/envs/test/lib/python3.7/site-packages (from pycocotools->mmdet==2.11.0) (52.0.0.post20210125)\n",
      "Installing collected packages: mmdet\n",
      "  Attempting uninstall: mmdet\n",
      "    Found existing installation: mmdet 2.11.0\n",
      "    Uninstalling mmdet-2.11.0:\n",
      "      Successfully uninstalled mmdet-2.11.0\n",
      "  Running setup.py develop for mmdet\n",
      "Successfully installed mmdet\n"
     ]
    }
   ],
   "source": [
    "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu102/torch1.8.0/index.html\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git\n",
    "!cd mmdetection && pip install -q -r requirements/build.txt && pip install -q -v -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Start ModelCI Service\n",
    "Then we can start our ModelCI service, you should at least set env variables once before starting. You can refer to [last notebook](https://github.com/cap-ntu/ML-Model-CI/blob/master/example/notebook/image_classification_model_deployment.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 14:52:13.343730: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-04-28 14:52:18,839 - ml-modelci Docker Container Manager - INFO - Container name=mongo-27508 stared\n",
      "2021-04-28 14:52:20,128 - ml-modelci Docker Container Manager - INFO - Container name=cadvisor-36896 started.\n",
      "2021-04-28 14:52:21,386 - ml-modelci Docker Container Manager - INFO - Container name=dcgm-exporter-23136 started.\n",
      "2021-04-28 14:52:22,517 - ml-modelci Docker Container Manager - INFO - gpu-metrics-exporter-90458 stared\n",
      "2021-04-28 14:52:23,028 - modelci backend - INFO - Uvicorn server listening on http://localhost:8000, check full log at /home/modelci/tmp/modelci.log\n"
     ]
    }
   ],
   "source": [
    "!modelci service init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build MMdetection Model\n",
    "### 2.1 Imports\n",
    "We should import the following functions:\n",
    "- preprocess_example_input: for generating tensor and meta info from example image file\n",
    "- build_model_from_cfg: for building model form config file and checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.core.export import preprocess_example_input, build_model_from_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Config\n",
    "\n",
    "We should either use a dict or config file for configuration of MMDetection model, to make things simple, we use a config file provided by MMDetection.\n",
    "\n",
    "Notice: \n",
    "\n",
    "- You may need to manually download pretrained model checkpoints from [MMDetection models zoo](https://github.com/open-mmlab/mmdetection/blob/master/docs/model_zoo.md).\n",
    "- Only a few MMdet models are able to converted into ONNX format, you can refer to [documentation](https://mmdetection.readthedocs.io/en/latest/tutorials/pytorch2onnx.html#list-of-supported-models-exportable-to-onnx) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'mmdetection/configs/retinanet/retinanet_r50_fpn_1x_coco.py'\n",
    "checkpoint_file = 'retinanet_r50_fpn_1x_coco_20200130-c2398f9e.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build Model\n",
    "Then we can build our MMdetection model based on the configuration above and the checkpoint file we already download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "model = build_model_from_cfg(config_file, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conversion, we need to modify forward function to provide the necessary **kwargs parameters such as img_metas.\n",
    "\n",
    "In order to obtain valid bbox data during the onnx tracing process, we also need to use a tensor generated from image file as model input instead of random tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_config = {\n",
    "    'input_shape': (1,3,224,224),\n",
    "    'input_path': 'mmdetection/demo/demo.jpg',\n",
    "    'normalize_cfg': {\n",
    "        'mean': (123.675, 116.28, 103.53),\n",
    "        'std': (58.395, 57.12, 57.375)\n",
    "        }\n",
    "}\n",
    "one_img, one_meta = preprocess_example_input(input_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "model.forward = partial(model.forward, img_metas=[[one_meta]], return_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Register Model\n",
    "We can convert the pytorch model above into optimized formats, such as ONNX through modelci\n",
    "\n",
    "### 3.1 Imports\n",
    "- modelci.hub.manager: for registering model into ModelHub\n",
    "- modelci.hub.utils: for generating model save path\n",
    "- modelci.types.models: for constructing model inputs paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from modelci.hub.registrar import register_model\n",
    "from modelci.hub.utils import generate_path_plain\n",
    "from modelci.types.models import MLModel\n",
    "from modelci.types.models.common import Engine, Task, Framework, Metric, ModelStatus, IOShape, DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = 'RetinaNet'\n",
    "framework = Framework.PyTorch\n",
    "engine = Engine.PYTORCH\n",
    "version = 1\n",
    "task = Task.Object_Detection\n",
    "model_save_path = generate_path_plain(architecture, task, framework, engine, version)\n",
    "\n",
    "if not Path.is_dir(Path(model_save_path).parent):\n",
    "    os.makedirs(Path(model_save_path).parent,exist_ok=True)\n",
    "torch.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Construct MLModel Instance\n",
    "\n",
    "Here are some parameters need to be specified before model conversion.\n",
    "- inputs: The model inputs info\n",
    "- outputs: The model outputs info\n",
    "- metric: The evaludation metric data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = MLModel(\n",
    "    weight=Path(model_save_path),\n",
    "    architecture=architecture,\n",
    "    dataset='COCO',\n",
    "    framework=framework,\n",
    "    engine=engine,\n",
    "    version=version,\n",
    "    metric={Metric.mAP: 0.365},\n",
    "    task=task,\n",
    "    inputs=[IOShape(name=\"input\", shape=[-1, 3, 204, 204], dtype=DataType.TYPE_FP32)],\n",
    "    outputs=[\n",
    "        IOShape(name=\"BBOX\", shape=[-1, 100, 5], dtype=DataType.TYPE_FP32),\n",
    "        IOShape(name=\"SCORE\", shape=[-1, 100], dtype=DataType.TYPE_FP32)\n",
    "    ],\n",
    "    model_status=[ModelStatus.PUBLISHED],\n",
    "    model_input = [one_img]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Register\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 17:01:16,961 - converter - WARNING - This model is not supported as torchscript format\n",
      "2021-04-28 17:01:16,961 - converter - WARNING - This model is not supported as torchscript format\n",
      "2021-04-28 17:01:16,961 - converter - WARNING - This model is not supported as torchscript format\n",
      "2021-04-28 17:01:16,961 - converter - WARNING - This model is not supported as torchscript format\n",
      "2021-04-28 17:01:16,964 - converter - INFO - Use cached model\n",
      "2021-04-28 17:01:16,964 - converter - INFO - Use cached model\n",
      "2021-04-28 17:01:16,964 - converter - INFO - Use cached model\n",
      "2021-04-28 17:01:16,964 - converter - INFO - Use cached model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[MLModel(architecture='RetinaNet', framework=<Framework.PyTorch: 1>, engine=<Engine.PYTORCH: 7>, version=1, dataset='COCO', metric={<Metric.mAP: 1>: 0.365}, task=<Task.Object_Detection: 1>, inputs=[IOShape(shape=[-1, 3, 204, 204], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)], outputs=[IOShape(shape=[-1, 100, 5], dtype=<DataType.TYPE_FP32: 11>, name='BBOX', format=<ModelInputFormat.FORMAT_NONE: 0>), IOShape(shape=[-1, 100], dtype=<DataType.TYPE_FP32: 11>, name='SCORE', format=<ModelInputFormat.FORMAT_NONE: 0>)], id=ObjectId('6089245cbf833faa785ce5ad'), parent_model_id=None, weight=Weight(__root__=ObjectId('60892459bf833faa785ce364')), profile_result=None, status=<Status.Unknown: 0>, model_input=None, model_status=[<ModelStatus.PUBLISHED: 0>], creator='modelci', create_time=datetime.datetime(2021, 4, 28, 9, 1, 5, 195276)),\n",
       " MLModel(architecture='RetinaNet', framework=<Framework.PyTorch: 1>, engine=<Engine.ONNX: 3>, version=1, dataset='COCO', metric={<Metric.mAP: 1>: 0.365}, task=<Task.Object_Detection: 1>, inputs=[IOShape(shape=[-1, 3, 204, 204], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)], outputs=[IOShape(shape=[-1, 100, 5], dtype=<DataType.TYPE_FP32: 11>, name='BBOX', format=<ModelInputFormat.FORMAT_NONE: 0>), IOShape(shape=[-1, 100], dtype=<DataType.TYPE_FP32: 11>, name='SCORE', format=<ModelInputFormat.FORMAT_NONE: 0>)], id=ObjectId('6089245fbf833faa785ce7f5'), parent_model_id=None, weight=Weight(__root__=ObjectId('6089245dbf833faa785ce5ae')), profile_result=None, status=<Status.Unknown: 0>, model_input=None, model_status=[<ModelStatus.CONVERTED: 1>], creator='modelci', create_time=datetime.datetime(2021, 4, 28, 9, 1, 5, 195276))]"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_model(mlmodel, convert=True, profile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, MLModelCI support auto conversion of PyTorch models into both torchscript and ONNX format, as a result.\n",
    "\n",
    "However, this model cannot be transformed into torchscript format, but supportive of ONNX format conversion, there could be serveral factors contributing to model conversion failture such as the model structure and code format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieve Model\n",
    "\n",
    "The following steps will retrieve the model we just registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelci.hub.manager import retrieve_model\n",
    "retrieved_models = retrieve_model(\n",
    "    architecture='RetinaNet',\n",
    "    framework=Framework.PyTorch,\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MLModel(architecture='RetinaNet', framework=<Framework.PyTorch: 1>, engine=<Engine.PYTORCH: 7>, version=1, dataset='COCO', metric={<Metric.mAP: 1>: 0.365}, task=<Task.Object_Detection: 1>, inputs=[IOShape(shape=[-1, 3, 204, 204], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)], outputs=[IOShape(shape=[-1, 100, 5], dtype=<DataType.TYPE_FP32: 11>, name='BBOX', format=<ModelInputFormat.FORMAT_NONE: 0>), IOShape(shape=[-1, 100], dtype=<DataType.TYPE_FP32: 11>, name='SCORE', format=<ModelInputFormat.FORMAT_NONE: 0>)], id=ObjectId('6089245cbf833faa785ce5ad'), parent_model_id=None, weight=Weight(__root__=ObjectId('60892459bf833faa785ce364')), profile_result=None, status=<Status.Unknown: 0>, model_input=None, model_status=[<ModelStatus.PUBLISHED: 0>], creator='modelci', create_time=datetime.datetime(2021, 4, 28, 9, 1, 5, 195000, tzinfo=<bson.tz_util.FixedOffset object at 0x7ff4c7957a50>)),\n",
       " MLModel(architecture='RetinaNet', framework=<Framework.PyTorch: 1>, engine=<Engine.ONNX: 3>, version=1, dataset='COCO', metric={<Metric.mAP: 1>: 0.365}, task=<Task.Object_Detection: 1>, inputs=[IOShape(shape=[-1, 3, 204, 204], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)], outputs=[IOShape(shape=[-1, 100, 5], dtype=<DataType.TYPE_FP32: 11>, name='BBOX', format=<ModelInputFormat.FORMAT_NONE: 0>), IOShape(shape=[-1, 100], dtype=<DataType.TYPE_FP32: 11>, name='SCORE', format=<ModelInputFormat.FORMAT_NONE: 0>)], id=ObjectId('6089245fbf833faa785ce7f5'), parent_model_id=None, weight=Weight(__root__=ObjectId('6089245dbf833faa785ce5ae')), profile_result=None, status=<Status.Unknown: 0>, model_input=None, model_status=[<ModelStatus.CONVERTED: 1>], creator='modelci', create_time=datetime.datetime(2021, 4, 28, 9, 1, 5, 195000, tzinfo=<bson.tz_util.FixedOffset object at 0x7ff4c7957a50>))]"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's no wonder we get two model objects here cause there is an addition ONNX format model created automatically during previous registering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'RetinaNet',\n",
       " 'framework': <Framework.PyTorch: 1>,\n",
       " 'engine': <Engine.PYTORCH: 7>,\n",
       " 'version': 1,\n",
       " 'dataset': 'COCO',\n",
       " 'metric': {<Metric.mAP: 1>: 0.365},\n",
       " 'task': <Task.Object_Detection: 1>,\n",
       " 'inputs': [IOShape(shape=[-1, 3, 204, 204], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'outputs': [IOShape(shape=[-1, 100, 5], dtype=<DataType.TYPE_FP32: 11>, name='BBOX', format=<ModelInputFormat.FORMAT_NONE: 0>),\n",
       "  IOShape(shape=[-1, 100], dtype=<DataType.TYPE_FP32: 11>, name='SCORE', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'id': ObjectId('6089245cbf833faa785ce5ad'),\n",
       " 'parent_model_id': None,\n",
       " 'weight': Weight(__root__=ObjectId('60892459bf833faa785ce364')),\n",
       " 'profile_result': None,\n",
       " 'status': <Status.Unknown: 0>,\n",
       " 'model_input': None,\n",
       " 'model_status': [<ModelStatus.PUBLISHED: 0>],\n",
       " 'creator': 'modelci',\n",
       " 'create_time': datetime.datetime(2021, 4, 28, 9, 1, 5, 195000, tzinfo=<bson.tz_util.FixedOffset object at 0x7ff4c7957a50>)}"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_models[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'RetinaNet',\n",
       " 'framework': <Framework.PyTorch: 1>,\n",
       " 'engine': <Engine.ONNX: 3>,\n",
       " 'version': 1,\n",
       " 'dataset': 'COCO',\n",
       " 'metric': {<Metric.mAP: 1>: 0.365},\n",
       " 'task': <Task.Object_Detection: 1>,\n",
       " 'inputs': [IOShape(shape=[-1, 3, 204, 204], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'outputs': [IOShape(shape=[-1, 100, 5], dtype=<DataType.TYPE_FP32: 11>, name='BBOX', format=<ModelInputFormat.FORMAT_NONE: 0>),\n",
       "  IOShape(shape=[-1, 100], dtype=<DataType.TYPE_FP32: 11>, name='SCORE', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'id': ObjectId('6089245fbf833faa785ce7f5'),\n",
       " 'parent_model_id': None,\n",
       " 'weight': Weight(__root__=ObjectId('6089245dbf833faa785ce5ae')),\n",
       " 'profile_result': None,\n",
       " 'status': <Status.Unknown: 0>,\n",
       " 'model_input': None,\n",
       " 'model_status': [<ModelStatus.CONVERTED: 1>],\n",
       " 'creator': 'modelci',\n",
       " 'create_time': datetime.datetime(2021, 4, 28, 9, 1, 5, 195000, tzinfo=<bson.tz_util.FixedOffset object at 0x7ff4c7957a50>)}"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_models[1].__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
