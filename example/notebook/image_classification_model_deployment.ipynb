{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation, Converting and Registering Image Classification Model by ModelCI\n",
    "\n",
    "This is a getting started tutorial for those who are new to ML-ModelCI, by the end of this tutorial, you will be able to: \n",
    "\n",
    "- Install ML-ModelCI.\n",
    "- Setup environment.\n",
    "- Start and stop ModelCI service.\n",
    "- Master basic usages of ML-ModelCI, such as model loading, registering,retrieving and converting.\n",
    "- Have a basic understanding of machine learning model lifecycle.\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "Here are some prequisities before installation\n",
    "\n",
    "- Python version: 3.7 or higher\n",
    "- Docker service installed and started\n",
    "- Manually install [TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html) if your linux distribution is not Ubuntu\n",
    "\n",
    "We can install the ModelCI python package based on <https://github.com/cap-ntu/ML-Model-CI/#installation-using--pip>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/cap-ntu/ML-Model-CI.git\n",
    "cd ML-Model-CI\n",
    "pip install -q ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start the ModelCI Service\n",
    "\n",
    "For the first time you start the ModelCI service, you need to setup all the environment variables in a single `.env` file by the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read env-backend.env ...\n",
      "Read env-mongodb.env ...\n",
      "Read env-frontend.env ...\n",
      "Write .env for backend with setup:\n",
      " {\n",
      "  \"PROJECT_NAME\": \"modelci\",\n",
      "  \"SERVER_HOST\": \"localhost\",\n",
      "  \"SERVER_PORT\": \"8000\",\n",
      "  \"SECRET_KEY\": \"2a6c03b9ca06cd8fc3cf506f0ba924cb735f15918d54758426fd7282366a5e19\",\n",
      "  \"MONGO_HOST\": \"localhost\",\n",
      "  \"MONGO_PORT\": \"27017\",\n",
      "  \"MONGO_USERNAME\": \"modelci\",\n",
      "  \"MONGO_PASSWORD\": \"modelci@2020\",\n",
      "  \"MONGO_DB\": \"modelci\",\n",
      "  \"MONGO_AUTH_SOURCE\": \"modelci\",\n",
      "  \"BACKEND_CORS_ORIGINS\": \"localhost:3333\"\n",
      "}\n",
      "Write .env for frontend with setup:\n",
      " {\n",
      "  \"HOST\": \"localhost\",\n",
      "  \"PORT\": \"3333\",\n",
      "  \"REACT_APP_BACKEND_URL\": \"localhost:8000\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# set environment variables \n",
    "!python scripts/generate_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start the modelci service by following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 00:58:48.608554: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-04-28 00:58:53,839 - ml-modelci Docker Container Manager - INFO - Container name=mongo-49205 stared\n",
      "2021-04-28 00:58:55,196 - ml-modelci Docker Container Manager - INFO - Container name=cadvisor-12065 started.\n",
      "2021-04-28 00:58:56,522 - ml-modelci Docker Container Manager - INFO - Container name=dcgm-exporter-5717 started.\n",
      "2021-04-28 00:58:57,853 - ml-modelci Docker Container Manager - INFO - gpu-metrics-exporter-31125 stared\n",
      "2021-04-28 00:58:58,377 - modelci backend - INFO - Uvicorn server listening on http://localhost:8000, check full log at /home/shanshan/tmp/modelci.log\n"
     ]
    }
   ],
   "source": [
    "!modelci service init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register ResNet50 Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load pre-trained resnet50 model from torchvision, you can refer to <https://pytorch.org/docs/stable/torchvision/models.html> for more examples of pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we specify five basic attributes of this model:\n",
    "- architecture\n",
    "- framework\n",
    "- engine\n",
    "- task\n",
    "- version\n",
    "\n",
    "and save the model in the generated path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from modelci.types.models.common import Engine, Task, Framework, Metric, ModelStatus, IOShape, DataType\n",
    "from modelci.types.models import MLModel\n",
    "from modelci.hub.utils import generate_path_plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = 'ResNet50'\n",
    "framework = Framework.PyTorch\n",
    "engine = Engine.PYTORCH\n",
    "version = 1\n",
    "task = Task.Image_Classification\n",
    "model_save_path = generate_path_plain(architecture, task, framework, engine, version)\n",
    "\n",
    "if not Path.is_dir(Path(model_save_path).parent):\n",
    "    os.makedirs(Path(model_save_path).parent, exist_ok=True)\n",
    "torch.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can build a MLModel instance and register it into modelhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = MLModel(\n",
    "    weight=Path(model_save_path),\n",
    "    architecture=architecture,\n",
    "    dataset='ImageNet',\n",
    "    framework=framework,\n",
    "    engine=engine,\n",
    "    version=version,\n",
    "    metric={Metric.acc: 0.80},\n",
    "    task=task,\n",
    "    inputs=[IOShape(name=\"input\", shape=[-1, 3, 224, 224], dtype=DataType.TYPE_FP32)],\n",
    "    outputs=[IOShape(name=\"output\", shape=[-1, 1000], dtype=DataType.TYPE_FP32)],\n",
    "    model_status=[ModelStatus.PUBLISHED]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-28 14:30:03,066 - converter - INFO - Torchscript format converted successfully\n",
      "2021-04-28 14:30:03,066 - converter - INFO - Torchscript format converted successfully\n",
      "2021-04-28 14:30:03,066 - converter - INFO - Torchscript format converted successfully\n",
      "2021-04-28 14:30:03,066 - converter - INFO - Torchscript format converted successfully\n",
      "2021-04-28 14:30:05,669 - converter - INFO - ONNX format converted successfully\n",
      "2021-04-28 14:30:05,669 - converter - INFO - ONNX format converted successfully\n",
      "2021-04-28 14:30:05,669 - converter - INFO - ONNX format converted successfully\n",
      "2021-04-28 14:30:05,669 - converter - INFO - ONNX format converted successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[MLModel(architecture='ResNet50', framework=<Framework.PyTorch: 1>, engine=<Engine.PYTORCH: 7>, version=1, dataset='ImageNet', metric={<Metric.acc: 0>: 0.8}, task=<Task.Image_Classification: 0>, inputs=[IOShape(shape=[-1, 3, 224, 224], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)], outputs=[IOShape(shape=[-1, 1000], dtype=<DataType.TYPE_FP32: 11>, name='output', format=<ModelInputFormat.FORMAT_NONE: 0>)], id=ObjectId('608900e967de5e180caf93ec'), parent_model_id=None, weight=Weight(__root__=ObjectId('608900e867de5e180caf9262')), profile_result=None, status=<Status.Unknown: 0>, model_input=None, model_status=[<ModelStatus.PUBLISHED: 0>], creator='shanshan', create_time=datetime.datetime(2021, 4, 28, 6, 29, 56, 12289)),\n",
       " MLModel(architecture='ResNet50', framework=<Framework.PyTorch: 1>, engine=<Engine.TORCHSCRIPT: 2>, version=1, dataset='ImageNet', metric={<Metric.acc: 0>: 0.8}, task=<Task.Image_Classification: 0>, inputs=[IOShape(shape=[-1, 3, 224, 224], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)], outputs=[IOShape(shape=[-1, 1000], dtype=<DataType.TYPE_FP32: 11>, name='output', format=<ModelInputFormat.FORMAT_NONE: 0>)], id=ObjectId('608900ee67de5e180caf9577'), parent_model_id=None, weight=Weight(__root__=ObjectId('608900ed67de5e180caf93ed')), profile_result=None, status=<Status.Unknown: 0>, model_input=None, model_status=[<ModelStatus.CONVERTED: 1>], creator='shanshan', create_time=datetime.datetime(2021, 4, 28, 6, 29, 56, 12289)),\n",
       " MLModel(architecture='ResNet50', framework=<Framework.PyTorch: 1>, engine=<Engine.ONNX: 3>, version=1, dataset='ImageNet', metric={<Metric.acc: 0>: 0.8}, task=<Task.Image_Classification: 0>, inputs=[IOShape(shape=[-1, 3, 224, 224], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)], outputs=[IOShape(shape=[-1, 1000], dtype=<DataType.TYPE_FP32: 11>, name='output', format=<ModelInputFormat.FORMAT_NONE: 0>)], id=ObjectId('608900f067de5e180caf9702'), parent_model_id=None, weight=Weight(__root__=ObjectId('608900ee67de5e180caf9578')), profile_result=None, status=<Status.Unknown: 0>, model_input=None, model_status=[<ModelStatus.CONVERTED: 1>], creator='shanshan', create_time=datetime.datetime(2021, 4, 28, 6, 29, 56, 12289))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelci.hub.manager import register_model\n",
    "register_model(mlmodel, convert=True, profile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve Models\n",
    "By default, Converter will automatically convert registered models into optimized formats,PyTorch model can be converted to TorchScipt and ONNX formats, so we can retrieve two models from ModelHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelci.hub.manager import retrieve_model\n",
    "retrieved_models = retrieve_model(\n",
    "    architecture='ResNet50',\n",
    "    framework=Framework.PyTorch,\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare detatiled information of these three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'architecture': 'ResNet50',\n",
       " 'framework': <Framework.PyTorch: 1>,\n",
       " 'engine': <Engine.PYTORCH: 7>,\n",
       " 'version': 1,\n",
       " 'dataset': 'ImageNet',\n",
       " 'metric': {<Metric.acc: 0>: 0.8},\n",
       " 'task': <Task.Image_Classification: 0>,\n",
       " 'inputs': [IOShape(shape=[-1, 3, 224, 224], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'outputs': [IOShape(shape=[-1, 1000], dtype=<DataType.TYPE_FP32: 11>, name='output', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'id': ObjectId('608900e967de5e180caf93ec'),\n",
       " 'parent_model_id': None,\n",
       " 'weight': Weight(__root__=ObjectId('608900e867de5e180caf9262')),\n",
       " 'profile_result': None,\n",
       " 'status': <Status.Unknown: 0>,\n",
       " 'model_input': None,\n",
       " 'model_status': [<ModelStatus.PUBLISHED: 0>],\n",
       " 'creator': 'shanshan',\n",
       " 'create_time': datetime.datetime(2021, 4, 28, 6, 29, 56, 12000, tzinfo=<bson.tz_util.FixedOffset object at 0x7f7b8b5abfd0>)}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "retrieved_models[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'architecture': 'ResNet50',\n",
       " 'framework': <Framework.PyTorch: 1>,\n",
       " 'engine': <Engine.TORCHSCRIPT: 2>,\n",
       " 'version': 1,\n",
       " 'dataset': 'ImageNet',\n",
       " 'metric': {<Metric.acc: 0>: 0.8},\n",
       " 'task': <Task.Image_Classification: 0>,\n",
       " 'inputs': [IOShape(shape=[-1, 3, 224, 224], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'outputs': [IOShape(shape=[-1, 1000], dtype=<DataType.TYPE_FP32: 11>, name='output', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'id': ObjectId('608900ee67de5e180caf9577'),\n",
       " 'parent_model_id': None,\n",
       " 'weight': Weight(__root__=ObjectId('608900ed67de5e180caf93ed')),\n",
       " 'profile_result': None,\n",
       " 'status': <Status.Unknown: 0>,\n",
       " 'model_input': None,\n",
       " 'model_status': [<ModelStatus.CONVERTED: 1>],\n",
       " 'creator': 'shanshan',\n",
       " 'create_time': datetime.datetime(2021, 4, 28, 6, 29, 56, 12000, tzinfo=<bson.tz_util.FixedOffset object at 0x7f7b8b5abfd0>)}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "retrieved_models[1].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'architecture': 'ResNet50',\n",
       " 'framework': <Framework.PyTorch: 1>,\n",
       " 'engine': <Engine.ONNX: 3>,\n",
       " 'version': 1,\n",
       " 'dataset': 'ImageNet',\n",
       " 'metric': {<Metric.acc: 0>: 0.8},\n",
       " 'task': <Task.Image_Classification: 0>,\n",
       " 'inputs': [IOShape(shape=[-1, 3, 224, 224], dtype=<DataType.TYPE_FP32: 11>, name='input', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'outputs': [IOShape(shape=[-1, 1000], dtype=<DataType.TYPE_FP32: 11>, name='output', format=<ModelInputFormat.FORMAT_NONE: 0>)],\n",
       " 'id': ObjectId('608900f067de5e180caf9702'),\n",
       " 'parent_model_id': None,\n",
       " 'weight': Weight(__root__=ObjectId('608900ee67de5e180caf9578')),\n",
       " 'profile_result': None,\n",
       " 'status': <Status.Unknown: 0>,\n",
       " 'model_input': None,\n",
       " 'model_status': [<ModelStatus.CONVERTED: 1>],\n",
       " 'creator': 'shanshan',\n",
       " 'create_time': datetime.datetime(2021, 4, 28, 6, 29, 56, 12000, tzinfo=<bson.tz_util.FixedOffset object at 0x7f7b8b5abfd0>)}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "retrieved_models[2].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Models\n",
    "We can convert models mannually. \n",
    "\n",
    "You can refer to <https://github.com/cap-ntu/ML-Model-CI/blob/master/docs/tutorial/convert.md> for more details.\n",
    "\n",
    "In the following example, we will convert ONNX model into TensorRT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/shanshan/.modelci/ResNet50/pytorch-onnx/image_classification/1.onnx')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ONNX model saved path\n",
    "onnx_path = retrieved_models[2].saved_path\n",
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/shanshan/.modelci/ResNet50/pytorch-trt/image_classification/1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set TensorRT format model save path\n",
    "architecture = 'ResNet50'\n",
    "framework = Framework.PyTorch\n",
    "version = 1\n",
    "task = Task.Image_Classification\n",
    "trt_model_save_path = generate_path_plain(architecture, task, framework, Engine.TRT, version)\n",
    "trt_model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ONNX file from path /home/shanshan/.modelci/ResNet50/pytorch-onnx/image_classification/1.onnx...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelci.hub.converter import convert\n",
    "convert(\n",
    "    model=retrieved_models[2].saved_path,\n",
    "    src_framework='onnx',\n",
    "    dst_framework='trt',\n",
    "    save_path=trt_model_save_path,\n",
    "    inputs=retrieved_models[2].inputs,\n",
    "    outputs=retrieved_models[2].outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Stop the ModelCI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 00:59:10.251530: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-04-28 00:59:12,892 - ml-modelci Docker Container Manager - INFO - Container name=gpu-metrics-exporter-31125 stopped.\n",
      "2021-04-28 00:59:13,535 - ml-modelci Docker Container Manager - INFO - Container name=cadvisor-12065 stopped.\n",
      "2021-04-28 00:59:14,859 - ml-modelci Docker Container Manager - INFO - Container name=mongo-49205 stopped.\n",
      "2021-04-28 00:59:14,994 - modelci backend - WARNING - No process is listening on port 8000\n"
     ]
    }
   ],
   "source": [
    "!conda activate test && modelci service stop"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Or  you can remove all the stoppped docker containers by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 00:58:27.911203: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-04-28 00:58:30,634 - ml-modelci Docker Container Manager - INFO - Container name=cadvisor-53037 stopped.\n",
      "2021-04-28 00:58:31,639 - ml-modelci Docker Container Manager - INFO - Container name=mongo-50811 stopped.\n",
      "2021-04-28 00:58:31,742 - modelci backend - WARNING - No process is listening on port 8000\n",
      "2021-04-28 00:58:31,796 - ml-modelci Docker Container Manager - INFO - Container a81962bf3b3d89e55a3588831b050cfd43639dc94a524402737a64982f428215 is removed.\n",
      "2021-04-28 00:58:31,847 - ml-modelci Docker Container Manager - INFO - Container 5d9576a66007d8434400fbaf1b2a0f710f3896e21704d30bfd4d1d8694285a8a is removed.\n",
      "2021-04-28 00:58:31,916 - ml-modelci Docker Container Manager - INFO - Container c4eabfb515aaafcd3497ae44ff59172714aa9bd58b92cbfd943aea5c4e44d542 is removed.\n",
      "2021-04-28 00:58:31,986 - ml-modelci Docker Container Manager - INFO - Container 19f1d9b6837945d142e3da22c5371d66fa7b09b595b3870eeef3466cf116f8c3 is removed.\n"
     ]
    }
   ],
   "source": [
    "!conda activate test && modelci service clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "```raw\n",
    "Copyright 2020 Nanyang Technological University, Singapore\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}